# Semi-Supervised Training for Detection of Personal-Protective-Equipment (PPE)

## Problem

We explained in detail at this [repository](https://github.com/thainguyentrong/gfl-ppe). You need watch it before reading on.

## Our implementation

### Semi-Supervised Training

<img src="resources/model.png" align="middle"/>

The availability of large-scale datasets and computational resources has allowed deep neural networks to achieve strong performance on a wide variety of tasks. However, training these networks requires a large number of labeled examples that are expensive to annotate and acquire. As an alternative, Semi-Supervised Learning methods have received growing attention. Yet, these advances have primarily focused on image classification, rather than object detection where bounding box annotations require more effort.

### Overall impression

<img src="resources/ssod.png" align="middle"/>

The nature of class-imbalance in object detection tasks impedes the usage of pseudo-labeling. Also, object detectors are far more complicated than image classifiers in terms of model architectures.

This [article](https://arxiv.org/abs/2102.09480) proposed an approach that jointly trains a Student and a slowly progressing Teacher in a mutually-beneficial manner, in which the Teacher generates pseudo-labels to train the Student, while the Teacher and the Student are given different augmented input images.

### Method

Our training process is as follows:

- Using labeled data to train the detector (and use Focal Loss - in this repository we use Generalized Focal Loss). The article calls this **burn-in** stage.
- **Student - Teacher mutual learning**: the first stage clones the detector obtained in burn-in stage into the Teacher model and Student model. Then, the corresponding unlabeled data will be weakly augmented and strongly augmented. Pseudo labels of weakly augmented data are generated by the Teacher model, and the Student model forward strongly augmented data with pseudo labels for training. During training, only the Student model is optimized via back-propagation, and the Teacher model is updated by gradually transferring the weights of continually learned Student model via **exponential moving average (EMA)**.

## Results

## Citation

```
@article{Liu2021UnbiasedTF,
  title={Unbiased Teacher for Semi-Supervised Object Detection},
  author={Yen-Cheng Liu and Chih-Yao Ma and Zijian He and Chia-Wen Kuo and Kan Chen and Peizhao Zhang and Bichen Wu and Zsolt Kira and P{\'e}ter Vajda},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.09480}
}
```
